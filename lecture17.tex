\documentclass[a4paper,english,12pt]{article}
\input{header}
\title{Lecture 17: Estimation Theory}
\date{March 10, 2016}
\author{}
\begin{document}
\maketitle
\maketitle
In this lecture , we will start our discussion on methods for finding estimators, and evaluatng these estimators.
\section{Introduction:}
In previous lectures, we studied about the problem of detection, which is nothing but deciding between two (or more) different hypothesis. In estimation, we estimate or guess the value of an unknown(point). This unknown need not to be a real number value, it can also be a vector or may take a range of interval. In our lecture, we will focus on \textit{point estimation}. Our purpose is to estimate a point, which will yield to the knowledge of entire population. Following is the definition of a \textit{point estimator}.\\

\begin{defn}{ A \textit{point estimator} is any function $W(X_1,X_2,...,X_n)$ of samples. That means, any statistic(like sample mean, sample variance) is a point estimator.}
\end{defn}

\section{Methods of Constructing Estimtors: }
In some cases our intuition lead us to a very good estimator. For example, estimating a parameter with it's sample analogue is usually reasonable. Like, the sample mean is a good estimator for the population mean, but this is not the case always. Sometimes our intuition lead us to a very bad estimator that seem to be correct. So we need a more methodical way of estimating parameters. Following are some methods of finding estimators.

\subsection{Method of Moments}
The method of moments is , perhaps, the oldest method of finding point estimators. It is quite simple to use and almost always yields some sort of estimate.

Let $X_1,X_2,...,X_n$ be a sample from a population with pdf or pmf $f(x|\theta_1,....,\theta_k)$. Methods of moment estimators are found by equating the first $k$ sample moments to the corresponding $k$ population moments, and solving the resulting system of simultaneous equations. More precisely, define
\begin{align}
 %m_1 &= \frac{1}{n}\sum\limits_{i=1}^n{X^1_i}, \,\,\,\,\,\,\,%\mu'_1,\\
%m_2 &= \frac{1}{n}\sum\limits_{i=1}^n{X^2_i}, \,\,\,\,\,\,\,%\mu'_2,\ \\
%&\vdots \nonumber\\
 m_j = \frac{1}{n}\sum\limits_{i=1}^n{X^j_i}, \,\,\,\,\,\,\,\mu'_j.\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\mathrm{for}\,\, j = 1,2,...,k
\end{align}  

The population moment $\mu'_j$ will typically be a function of $\theta_1,....,\theta_k$, say $\mu'_j(\theta_1,....,\theta_k)$. The method of moments estimator $(\tilde\theta_1,....,\tilde\theta_k)$ of $\theta_1,....,\theta_k$ is obtained by solving the following sytem of equations for $(\theta_1,....,\theta_k)$ in terms of $m_1,...,m_k$:
\begin{align}
%m_1 &= \mu'_1(\theta_1,....,\theta_k),\\
%m_2 &= \mu'_2(\theta_1,....,\theta_k),\\
%\vdots\nonumber\\
m_j = \mu'_j(\theta_1,....,\theta_k). \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\mathrm{for}\,\, j = 1,2,...,k
\end{align} 

Definetely there will be some error in our estimate of the parameter but as we will increase the number of samples, the error will reduce.

\begin{exmp}\textbf{(Normal method of moments)}\\
Let samples $X_1,X_2,...,X_n$ are independent and gaussian distributed with mean $\theta$ and variance $\sigma^2$. So our parameters for estimation are, $\theta_1=\theta$ and $\theta_2=\sigma^2$. We have $m_1 = \bar{X}, m_2 = (1/n)\sum{X^2_i}, \mu'_1=\theta, \mu'_2 = \theta^2 + \sigma^2 $, and hence we must solve 
\begin{align}
\bar{X}=\theta,\,\,\,\,\,(1/n)\sum{X^2_i} = \theta^2 + \sigma^2
\end{align} 
Solving for $\theta$ and $\sigma^2$ yields the method of moments estimators
\begin{align}
\tilde{\theta} = \bar{X},\,\,\,\,\,\tilde{\sigma^2}=(1/n)\sum{X^2_i} - \bar{X}^2=(1/n)\sum{(X_i - \bar{X})}^2
\end{align} 
\end{exmp}

\begin{exmp}\textbf{(Binomial method of moments)}
Let samples $X_1,X_2,...,X_n$ are independent and binomial distributed with parameters$(k,p)$, that is,
\begin{align}
P(X_i=x|k,p) = {{k}\choose{x}}p^x(1-p)^{k-x}\,\,\,\,\,\,x=0,1,...,k.
\end{align} 

Here we want the point estimator for both unknown parameters $k$ and $p$.
Equating the first two sample moments to their corresponding population moments yields the system of equations
\begin{align}
\bar{X} &= kp,\\
\frac{1}{n}\sum{X_i}^2 &= kp(1-p) + k^2p^2
\end{align} 
Now we can solve it  for $k$ and $p$. Putting value of $kp$ from eq.$(10)$ to eq.$(11)$, we will get,
$$\frac{1}{n}\sum{X_i}^2 = \bar{X}(1-p) + \bar{X}^2$$
\begin{align}
\tilde{p}=1-\frac{(1/n)\sum{(X_i - \bar{X})}^2}{\bar{X}} ,     \,\,\,\,\,\tilde{k} = \frac{\bar{X}^2}{\bar{X}-(1/n)\sum{(X_i - \bar{X})}^2}
\end{align} 
Here, $\tilde{p}$ and $\tilde{k}$ are the estimators of $p$ and $k$ respectively.

By observing $\tilde{p}$ and $\tilde{k}$, we can say that it is possible to get the negative estimates of $p$ and $k$ which, of course, must be positive numbers, Which implies that it is not necessary to coincide the range of estimator to the range of parameter it is estimating. However, we may reduce the probability of happening of such event by taking large number of observables.   
\end{exmp}

\subsection{Maximum Likelihood Estimators}

Let $X_1,X_2,...,X_n$ are an iid sample from a population with pdf or pmf $f(x|\theta_1,....,\theta_k)$, the likelihood function is defined by
\begin{align}
L(\mathrm{x}|\theta)=L(x_1,...,x_n|\theta_1,...,\theta_k)=\prod\limits_{i=1}^n f(x_i|\theta_1,....,\theta_k),\,\,\,\,\,\theta \in \Theta\subseteq\mathbb{R}^k
\end{align} 
\begin{defn}{Given observations $x_1,x_2,...,x_n$, a maximum likelihood estimate of $\theta$ is an element of $\mathrm{argmax}_{\theta \in \Theta}L_{\theta}(x)$.}


Likelihood of $x_1,x_2,...,x_n$ under $p(x_1,...,x_n|\theta)$ is
\begin{align}
L_{\theta}(x) = \mathbb{P}(X_1=x_1,...,X_n=x_n|\theta)=\prod\limits_{i=1}^n f(x_i|\theta)
\end{align} 
\end{defn}
\textbf{Remarks:}
\begin{enumerate}
\item By it's construction, the range of the MLE(maximum likelihood estimate) coincides with the range of the parameter.
\item MLE is the solution to an optimizing problem, i.e. an optimizer. 
\end{enumerate}

\begin{exmp}\textbf{Normal likelihood}
Let samples $X_1,X_2,...,X_n$ are independent and gaussian distributed with mean $\theta$ and variance $\sigma^2$. We want to get an estimate of $\theta$ and $\sigma^2$. For that, we need to solve :

\begin{align}
\mathrm{argmax}_{(\theta,\sigma^2)}\prod\limits_{i=1}^n f(x_i|\theta,\sigma^2)&= \mathrm{argmax}\sum\limits_{i=1}^n \mathrm{log} f(x_i|\theta,\sigma^2)\\
 =&\mathrm{argmax}\sum\limits_{i=1}^n\{(-1/2)\mathrm{log}(2\pi\sigma^2)-(1/2\sigma^2)(x_i-\theta)^2\}\nonumber\\
%\begin{eqnarray}
=&\mathrm{argmax}\{(-n/2)\mathrm{log}(2\pi)-(n/2)\mathrm{log}(\sigma^2)-\nonumber\\
&(1/2\sigma^2)\sum\limits_{i=1}^n(x_i-\theta)^2\}\nonumber\\
%\end{eqnarray} 
=&g(\theta,\sigma^2)
\end{align}


At optimality : 
\begin{align}
\nabla{g}(\theta,\sigma^2)=0
\end{align} 
To get the estimate of $\theta$, we need to make the partial differentiation of ${g}(\theta,\sigma^2)$ w.r.t. $\theta$, equal to 0.
\begin{align}
\frac{\partial g}{\partial \theta}&=0\\
\frac{1}{\sigma^2}\sum\limits_{i=1}^n(x_i-\theta)&=0\\
\tilde{\theta}&=\frac{1}{n}\sum\limits_{i=1}^n x_i\\
\frac{\partial^2 g}{\partial \theta^2}&=-\frac{n}{\sigma^2}
\end{align}
Hence,the second derivative is negative at $\theta$=$\tilde{\theta}$, so we can say that
 $\tilde{\theta}$ is the MLE of $\theta$.

Similarly, to get the estimate of $\sigma^2$ ,
\begin{align}
\frac{\partial g}{\partial \sigma^2}&=0\\
\frac{-n}{2\sigma^2}+\frac{1}{2(\sigma^2)^2}\sum\limits_{i=1}^n(x_i-\tilde{\theta)}^2&=0\nonumber\\
\tilde{\sigma^2}&=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\tilde{\theta)}^2
\end{align}
Let $\sigma^2=t$ ,$\tilde{\sigma^2}=\tilde{t}$
\begin{align}
{\left(\frac{\partial^2 g}{\partial t^2}\right)}_{t=\tilde{t}}&=\, \frac{n}{2\tilde{t}^2}-\frac{1}{\tilde{t}^3}\sum\limits_{i=1}^n(x_i-\tilde{\theta)}^2\\
&=-\frac{n^3}{2\left[\sum\limits_{i=1}^n(x_i-\tilde{\theta)}^2\right]^2}
\end{align}
Hence, $\tilde{\sigma^2}$ is the likelihood estimate of the $\sigma^2$.

\end{exmp}

\begin{exmp}\textbf{Bernoulli MLE}
Let $X_1X_2...X_n$ be iid Bernoulli($p$). Then the likelihood function is
\begin{align}
L(\textbf{x}|p)=\prod\limits_{i=1}^n p^{x_i}(1-p)^{1-x_i}=p^{y}(1-p)^{n-y}
\end{align} 


where ${y}=\sum x_i$       . it is much easier to differentiate the log likelihood 
$$\mathrm{log}L(\textbf{x}|p)={y}\mathrm{log}p + (n-{y}	)\mathrm{log}(1-p)$$


If $0<y<n$     , differentiating $\mathrm{log}L(\textbf{x}|p)$    and setting the result equal to $0$ give the solution, $\tilde{p}=y/n$   . It is also straightforward to verify that $y/n$   is the global maximum in this case. If $y=0$   or $y=n$, then
\begin{align}
\mathrm{log}L(\textbf{x}|p) = 
\begin{cases}
n\mathrm{log}(1-p)\,\,\,\,\,\,\,if\,\,\, y=0\\
n\mathrm{log}p\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,if\,\,\,\, y=n.\\
\end{cases}
\end{align}



In either case $\mathrm{log}L(\textbf{x}|p)$   is a monotone function of $p$, and it is again straightforward to verify that $\tilde{p}=y/n$  in each case.Thus, we have shown that $\sum {X_i}/n$  is the MLE of $p$.

\end{exmp}

\subsection{Bayes Estimators}

In the Bayesian approach parameter $\theta$ is considered to be a quantity whose variation can be described by a probability distribution (called the prior distribution). This is a subjective distribution, based on the experimenter's belief, and is formulated before the data are seen (hence the name prior distribution). A sample is then taken from a population indexed by $\theta$ and the prior distribution is updated with this sample information. The updated prior is called the posterior distribution.  This updating is done with the use of Baye's Rule, hence the name Bayesian statistics.

Given $\{f(x|\theta):\theta \in \Theta\}$, underlying assumption is that $\theta \in \Theta$ is randomly chosen from a prior distribution $\pi$ over $\Theta$, and therefore $X_1,X_2,...,X_n$ are iid over distribution $f(\textbf{x}|\theta)$.\\

\textbf{Note:}
 Choice of prior is subjective i.e. upto designer.\\

If $x_1,x_2,...,x_n$ are the observed samples, construct the posterior distribution for $\theta \in \Theta$ using Bayes rule.
\begin{align}
\pi(\theta|(x_1,x_2,...,x_n))=\frac{\pi(\theta)f(x_1,x_2,...,x_3|\theta)}{m(x_1,x_2,...,x_n)}
\end{align} 
where $m(\textbf{x})$ is the marginal distribution of $\textbf{X}$, that is,
\begin{align}
m(x_1,x_2,...,x_n)=\int_{\Theta}\pi(\theta')f(x_1,x_2,...,x_3|\theta')d\theta'
\end{align} 

One can write down possible estimators depending on the posterior distribution:
\begin{enumerate}
\item Mode
\item Expectation
\item Median
\end{enumerate}

\begin{exmp}\textbf{Bernoulli Bayes Estimators}
Let $X_1,X_2,...,X_n$ be iid Bernoulli($\theta$), $\theta \in [0,1]$.

Let prior has $\mathrm{Beta}(a,b)$ distribution then,
\begin{align}
\pi(\theta) = 
\begin{cases}
 \displaystyle{\frac{\theta^{a-1}(1-\theta)^{b-1}}{\beta(a,b)}},       \,\,\,\,\,\,\,if\,\,\,\theta \in [0,1] \\
0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,,\,\,\,\,\,\,\mathrm{otherwise}.\\
\end{cases}
\end{align} 

Where
\begin{align}
\beta(a,b) = \displaystyle{\int\limits_0^1 x^{a-1}(1-x)^{b-1}dx}
\end{align}
Let consider posterior distribution,
\begin{eqnarray}
\pi(\theta|(x_1,x_2,...,x_n))=&\displaystyle{\frac{\pi(\theta)f(x_1,x_2,...,x_n|\theta)}{m(x_1,x_2,...,x_n)}}\\
=&\displaystyle{ \frac{1}{m(\textbf{x})}\prod\limits_{i=1}^n \frac{\theta^{a-1}(1-\theta)^{b-1}}{\beta(a,b)} {\theta^{x_i}(1-\theta)^{1-x_i}}}\nonumber\\
=&\displaystyle{\frac{\theta^{\sum\limits_i {x_i}+a-1}(1-\theta)^{n-\sum\limits_i x_i +b-1}}{\beta(a,b)m(\textbf{x})}}\nonumber\\
\end{eqnarray}

Which is a $\mathrm{Beta}\left(\sum\limits_i {x_i}+a,n-\sum\limits_i x_i +b\right)$ distribution.  

\end{exmp}
Now we have posterior distribution so possible estimators :

\begin{enumerate}
\item Mode\,\,\,=\,\,\,$\displaystyle{\frac{\sum\limits_i {x_i}+a-1}{a+b+n-2}}$
\item Expectation\,\,\,=\,\,\,$\displaystyle{\frac{\sum\limits_i {x_i}+a}{a+b+n}}$
%\item Median
\end{enumerate}

\begin{defn}{Let $F$ denote the class of pdfs or pmfs
 $\{f(x|\theta):\theta \in \Theta\}$
 A class $\prod$ of prior distributions on $\Theta$ is a \textit{conjugate} family for $F$ if the posterior distribution is in the class $\prod$ for all $f \in F$, all priors in $\prod$.}
\end{defn}
\end{document}